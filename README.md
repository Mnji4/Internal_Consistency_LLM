# Internal_Consistency_LLM
项目概要：基于简单问法一致性的自我微调
1. 背景与核心假设
核心理念：不引入外部“上帝视角”或新知识，而是通过挖掘模型自身的潜力（Internal Capability）来达到性能上限。
一致性假设：一个完善的智能体应当具有逻辑自洽性。对于同一个事实，无论问法如何变化（简单 vs 复杂），答案应当一致。
关键策略：利用简单直接的问法（Base Prompt）更容易诱导模型输出正确答案这一特性，将其生成的答案作为“伪真值”（Pseudo-Ground Truth），去修正模型在复杂/对抗性问法（Perturbed Prompt）下的错误回答。
价值：实现一种低成本、自监督（Self-Supervised）的模型对齐与能力增强。
2. 执行计划 (To-Do List)
步骤一：数据与基准选择 (Benchmark Selection)
选取逻辑链条清晰、客观可验证的任务，避免模棱两可的开放性问答。
数学推理 (Math)：推荐 GSM8K 或 MATH 的子集。
理由：计算结果唯一，逻辑路径刚性，便于自动化判断一致性。
逻辑推理 (Logic)：推荐 RuleTaker 或 Folio。
理由：基于明确规则的“A推出B”形式，非常适合测试传递性和矛盾律。
步骤二：构造“简单-复杂”样本
对于数据集中的每一个原始问题（Query），构造两类变体：
简单问法 (Anchor/Simple)：
去除冗余信息。
使用最直接的命令式语气（例如：“直接计算结果”、“判断对错”）。
作用：获取高置信度的参考答案。
复杂/扰动问法 (Target/Complex)：
逻辑干扰：增加无关上下文（例如：“今天是周三，请计算...”）。
形式变化：改变语序、使用反讽语气、复杂句式嵌套。
推理链延长：将一步推理拆解为多步追问。
作用：诱发模型产生不一致（错误）的推理。
步骤三：推理与一致性筛选 (Inference & Filtering)
让模型分别回答上述两种问法：
获取参考答案：多次运行“简单问法”，获得答案 A1。
获取测试答案：多次运行“复杂问法”，获得答案 A2。
一致性校验：
如果 A1 与 A2 逻辑含义一致：丢弃（说明模型对此知识点已鲁棒，无需微调）。
如果 A1 与 A2 不一致（冲突）：
假设：A1 是正确的（基于你的核心策略）。
构造训练数据：输入是“复杂问法”，标签（Target）是 A1（简单问法生成的答案）。
步骤四：微调与评估 (Fine-tuning & Evaluation)
微调 (SFT)：使用步骤三筛选出的数据（Input: 复杂问法 -> Output: 简单问法得出的正确答案）对模型进行微调。
验证目标：
鲁棒性提升：模型在复杂问法下的准确率是否向简单问法看齐。
内部一致性：再次进行扰动测试，查看回答是否不再发生冲突。